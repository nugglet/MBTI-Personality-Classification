{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Data Preprocessing.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LCzc_0GdasaV"},"source":["import re\n","import pandas as pd\n","import numpy as np\n","\n","from tqdm import tqdm\n","\n","from sklearn import utils\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","from sklearn.multioutput import MultiOutputClassifier\n","\n","import torch\n","import time\n","import torch as T\n","device = T.device(\"cuda\")\n","\n","import gensim\n","from gensim.models.doc2vec import TaggedDocument\n","from gensim.models import doc2vec\n","from sklearn.svm import LinearSVC\n","\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras import backend as K\n","from keras.models import load_model\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n","from keras.layers import Flatten, LSTM\n","from keras.models import Model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uf_NCrjDULs-"},"source":["### Importing edited dataset from Github\n","The 'type' column is separated into 4 columns containing a letter for each aspect of the MBTI personality. "]},{"cell_type":"markdown","metadata":{"id":"1YK5ol-Yyw5H"},"source":["### Removing links and symbols from the 'posts' column"]},{"cell_type":"code","metadata":{"id":"NRzgiw1TBFxm"},"source":["def clear_text(data):\n","    data_length = []\n","    cleaned_text = []\n","    \n","    for sentence in data.posts:\n","        sentence = sentence.lower()\n","        # remove links from text\n","        sentence = re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',sentence)\n","        # remove other symbols\n","        sentence= re.sub('[^0-9a-z]',' ',sentence)\n","\n","        data_length.append(len(sentence.split()))\n","        cleaned_text.append(sentence)\n","    return cleaned_text, data_length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"bGMDOGZ4wvEW","executionInfo":{"status":"ok","timestamp":1638115848324,"user_tz":-480,"elapsed":4319,"user":{"displayName":"Praveen Rajesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizpZmwRdCqXmEa2XyOrEiebe2d-QcDM8tLhqP7ug=s64","userId":"03949090859866387122"}},"outputId":"96708c8a-9455-4507-c46f-ec76ab4e8f86"},"source":["df = pd.read_csv(\"https://raw.githubusercontent.com/zappocalypse/jubilant-meme/main/mbti_3_compressed.csv\")\n","# df = df.iloc[:500 , 1:]\n","df = df[[\"type\", \"spell_corrected\", \"is_I\",\"is_E\",\"is_N\",\"is_S\",\"is_F\",\"is_T\",\"is_J\",\"is_P\"]]\n","df = df.rename({'spell_corrected': 'posts'}, axis=1)\n","\n","df['cleaned_posts'], post_length = clear_text(df)\n","df.drop(columns=['is_E', 'is_S', 'is_F', 'is_P', 'type', 'posts'], inplace=True)\n","\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>is_I</th>\n","      <th>is_N</th>\n","      <th>is_T</th>\n","      <th>is_J</th>\n","      <th>cleaned_posts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>and into moments sportscaster not top ten play...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>i m finding the lack of me in these posts very...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>good one course to which i say i know that s m...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>dear into i enjoyed our conversation the other...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>you re fired that s another silly misconceptio...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   is_I  is_N  is_T  is_J                                      cleaned_posts\n","0     1     1     0     1  and into moments sportscaster not top ten play...\n","1     0     1     1     0  i m finding the lack of me in these posts very...\n","2     1     1     1     0  good one course to which i say i know that s m...\n","3     1     1     1     1  dear into i enjoyed our conversation the other...\n","4     0     1     1     1  you re fired that s another silly misconceptio..."]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"phcxezu54ri_"},"source":["### Doc2Vec\n"]},{"cell_type":"code","metadata":{"id":"CwM0klwWJZ0h"},"source":["def label_sentences(corpus, label_type):\n","    \"\"\"\n","    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n","    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n","    a dummy index of the post.\n","    \"\"\"\n","    labeled = []\n","    for i, v in enumerate(corpus):\n","        label = label_type + '_' + str(i)\n","        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n","    return labeled\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.cleaned_posts, df.iloc[:,:4], random_state=0, \n","                                                    test_size=0.3)\n","X_train_tagged = label_sentences(X_train, 'Train')\n","X_test_tagged = label_sentences(X_test, 'Test')\n","all_data = X_train_tagged + X_test_tagged"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4slPcOZTMfN-","executionInfo":{"status":"ok","timestamp":1636875028118,"user_tz":-480,"elapsed":31987,"user":{"displayName":"Miles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPrMEtX6MAT4jqSdS9ag5dXfJBaULexZ_hoFjExrM=s64","userId":"17256609399660266883"}},"outputId":"d2312e9a-5523-49d5-d115-531d71d2be6a"},"source":["model_dbow = doc2vec.Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, \n","                     min_alpha=0.065)\n","model_dbow.build_vocab([x for x in tqdm(all_data)])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 8675/8675 [00:00<00:00, 1293709.77it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sl7lqecWMumv","executionInfo":{"status":"ok","timestamp":1636875783873,"user_tz":-480,"elapsed":755764,"user":{"displayName":"Miles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPrMEtX6MAT4jqSdS9ag5dXfJBaULexZ_hoFjExrM=s64","userId":"17256609399660266883"}},"outputId":"5951ab16-700d-46bc-d04e-9950018d4dd5"},"source":["for epoch in range(50):\n","    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), \n","                     total_examples=len(all_data), \n","                     epochs=1)\n","    model_dbow.alpha -= 0.002\n","    model_dbow.min_alpha = model_dbow.alpha"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 8675/8675 [00:00<00:00, 1621461.11it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2730625.68it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2474368.39it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2801477.30it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1422868.26it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2747533.58it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2896019.36it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2819058.43it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 3091383.79it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1118145.95it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1211157.29it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 3037955.01it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 3235712.51it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2800399.23it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2814696.93it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2940724.74it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2528181.43it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2692237.31it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2509870.12it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2793733.66it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1378764.20it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1459920.04it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1048455.14it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2919020.23it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2874286.06it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2984626.95it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1167327.15it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2626928.54it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2709075.07it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1623921.59it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2942151.47it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2530995.21it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2954574.68it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2960103.09it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2904573.10it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2784966.49it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1313748.82it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2790091.80it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2752105.53it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 3006079.58it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2877695.92it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2862527.51it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2072426.22it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2839296.70it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 1376156.85it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2927004.04it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2532933.32it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2706455.46it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2902487.81it/s]\n","100%|██████████| 8675/8675 [00:00<00:00, 2739465.98it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"dsl13g_gM1e_"},"source":["def get_vectors(model, corpus_size, vectors_size, vectors_type):\n","    \"\"\"\n","    Get vectors from trained doc2vec model\n","    :param doc2vec_model: Trained Doc2Vec model\n","    :param corpus_size: Size of the data\n","    :param vectors_size: Size of the embedding vectors\n","    :param vectors_type: Training or Testing vectors\n","    :return: list of vectors\n","    \"\"\"\n","    vectors = np.zeros((corpus_size, vectors_size))\n","    for i in range(0, corpus_size):\n","        prefix = vectors_type + '_' + str(i)\n","        vectors[i] = model.docvecs[prefix]\n","    return vectors\n","\n","train_vectors_dbow = get_vectors(model_dbow, len(X_train_tagged), 300, 'Train')\n","test_vectors_dbow = get_vectors(model_dbow, len(X_test_tagged), 300, 'Test')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jbF6n7axPBII"},"source":["# Multilabel logistic regression"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oUz0LAvrPAhS","executionInfo":{"status":"ok","timestamp":1636875784552,"user_tz":-480,"elapsed":682,"user":{"displayName":"Miles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPrMEtX6MAT4jqSdS9ag5dXfJBaULexZ_hoFjExrM=s64","userId":"17256609399660266883"}},"outputId":"7bd888fe-8dfc-408b-c868-390fcef4f5e3"},"source":["# predicts labels INTJ\n","# i.e I:1 E:0 || N:1 S:0 || T: 1 F:0 || J:1 P:0\n","\n","clf = MultiOutputClassifier(estimator=LogisticRegression(n_jobs=1, C=1e5, max_iter=10000)).fit(train_vectors_dbow, y_train)\n","y_pred = clf.predict(test_vectors_dbow)\n","print(y_pred[:10])\n","\n","# logreg = LogisticRegression(n_jobs=1, C=1e5, max_iter=10000, multi_class='multinomial')\n","# logreg.fit(train_vectors_dbow, y_train)\n","\n","# logreg = logreg.fit(train_vectors_dbow, y_train)\n","# y_pred = logreg.predict(test_vectors_dbow)\n","# print(y_pred[:10])\n","\n","print('accuracy %s' % accuracy_score(y_pred, y_test))\n","print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 1 0 0]\n"," [1 1 0 1]\n"," [1 1 0 1]\n"," [1 1 1 0]\n"," [1 1 0 0]\n"," [1 1 0 0]\n"," [1 1 0 0]\n"," [1 1 1 1]\n"," [1 1 1 1]\n"," [1 1 0 0]]\n","accuracy 0.4721475220898963\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.92      0.89      2017\n","           1       0.91      0.97      0.94      2264\n","           2       0.81      0.80      0.81      1195\n","           3       0.70      0.62      0.66      1054\n","\n","   micro avg       0.85      0.87      0.86      6530\n","   macro avg       0.82      0.83      0.82      6530\n","weighted avg       0.84      0.87      0.85      6530\n"," samples avg       0.85      0.87      0.83      6530\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","metadata":{"id":"MjJPXLM6hk_D"},"source":["## Trying the multilabel methods from:\n","with TF Keras\n","\n","https://medium.com/swlh/multi-label-text-classification-with-scikit-learn-and-tensorflow-257f9ee30536"]},{"cell_type":"code","metadata":{"id":"aSJyUKvvj4-m"},"source":["num_classes = y_train.shape[1]\n","maxlen = 200\n","\n","tokenizer = Tokenizer(num_words=5000, lower=True)\n","tokenizer.fit_on_texts(df['cleaned_posts'])\n","max_words = len(tokenizer.word_index) + 1\n","# sequences = tokenizer.texts_to_sequences(df['cleaned_posts'])\n","# x = pad_sequences(sequences, maxlen=maxlen)\n","\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7QuMdIHKwa0","executionInfo":{"status":"ok","timestamp":1638115868677,"user_tz":-480,"elapsed":9,"user":{"displayName":"Praveen Rajesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizpZmwRdCqXmEa2XyOrEiebe2d-QcDM8tLhqP7ug=s64","userId":"03949090859866387122"}},"outputId":"3cba679f-31f6-4def-a252-69829e7eb729"},"source":["# calculate class weights\n","weights = {}\n","count = 0\n","for column in df.iloc[:, :4]: \n","  # from sklearn's implementation of calculating class weights\n","  # calculates weights of each class\n","  weight = df.shape[0]/(2 * np.bincount(df[column]))\n","  weights[count] = weight[0]\n","  count += 1\n","\n","print(weights)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 2.1698349174587293, 1: 3.6236424394319133, 2: 0.924051981252663, 3: 0.8276092348788399}\n"]}]},{"cell_type":"code","metadata":{"id":"jirEWtnKkwea"},"source":["def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-n7a4L1rcXqy"},"source":["# Basic feed forward nn\n","def basic_model(n_inputs, n_outputs):\n","  callbacks = [ReduceLROnPlateau(), ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)]\n","\n","  model = Sequential()\n","  model.add(Embedding(n_inputs, 20, input_length=maxlen))\n","  #model.add(Dropout(0.2))\n","  model.add(GlobalMaxPool1D())\n","  model.add(Dense(n_outputs, activation='sigmoid'))\n","\n","  model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"id":"pANsE7cQdLSw","executionInfo":{"status":"error","timestamp":1638115870070,"user_tz":-480,"elapsed":5,"user":{"displayName":"Praveen Rajesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizpZmwRdCqXmEa2XyOrEiebe2d-QcDM8tLhqP7ug=s64","userId":"03949090859866387122"}},"outputId":"4e2becdf-2ed4-4150-c1eb-7ff5577e93de"},"source":["model = basic_model(max_words, 4)\n","history = model.fit(X_train, y_train.to_numpy(),\n","                    epochs=30,\n","                    class_weight=weights,\n","                    batch_size=32,\n","                    validation_split=0.3,\n","                    callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-fd75ce86dc95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=callbacks)\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'callbacks' is not defined"]}]},{"cell_type":"code","metadata":{"id":"GRiTyAAIjvWA"},"source":["# basic model without weights initialization\n","model = basic_model(max_words, 4)\n","history = model.fit(X_train, y_train.to_numpy(),\n","                    epochs=30,\n","                    batch_size=32,\n","                    validation_split=0.3,\n","                    callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-L3VVSM0Gnmw"},"source":["## Using PyTorch\n","\n"]},{"cell_type":"code","metadata":{"id":"n4VAHjO4k5E2"},"source":[""],"execution_count":null,"outputs":[]}]}
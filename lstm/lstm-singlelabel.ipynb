{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch, re, string\nimport spacy, nltk\nfrom collections import Counter, OrderedDict\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors, vocab\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, multilabel_confusion_matrix\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-05T13:00:54.627870Z","iopub.execute_input":"2021-12-05T13:00:54.629115Z","iopub.status.idle":"2021-12-05T13:01:08.115867Z","shell.execute_reply.started":"2021-12-05T13:00:54.629075Z","shell.execute_reply":"2021-12-05T13:01:08.114383Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"https://raw.githubusercontent.com/zappocalypse/jubilant-meme/main/mbti_3_compressed.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:01:08.120854Z","iopub.execute_input":"2021-12-05T13:01:08.123508Z","iopub.status.idle":"2021-12-05T13:01:12.364506Z","shell.execute_reply.started":"2021-12-05T13:01:08.123465Z","shell.execute_reply":"2021-12-05T13:01:12.363219Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Spacy tokenizer\ntokenizer = get_tokenizer('spacy', language='en_core_web_sm') \n\nVOCAB_SIZE = 20000\n\n# build the vocab\ncounter = Counter()\nfor i, line in enumerate(df.spell_corrected):\n    counter.update(tokenizer(line))\n    \nordered_dict = OrderedDict(counter.most_common()[:VOCAB_SIZE])\nvoc = vocab(ordered_dict)\n\n# insert special tokens and set default index to 'unknown'\nvoc.insert_token('<PAD>', 0)\nvoc.insert_token('<UNK>', 1)\nvoc.set_default_index(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:01:12.366615Z","iopub.execute_input":"2021-12-05T13:01:12.366971Z","iopub.status.idle":"2021-12-05T13:01:35.071802Z","shell.execute_reply.started":"2021-12-05T13:01:12.366930Z","shell.execute_reply":"2021-12-05T13:01:35.070602Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n# tokenize dataset\nTRUNCATE_LEN = 1000\ndf['truncated'] = df.spell_corrected.apply(lambda x: \" \".join(x.split()[:TRUNCATE_LEN]))\ndf['tokenized'] = df.truncated.apply(lambda x: voc(tokenizer(x)))\ndf['tok_length'] = df.tokenized.apply(lambda x: len(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:01:35.074691Z","iopub.execute_input":"2021-12-05T13:01:35.074911Z","iopub.status.idle":"2021-12-05T13:01:48.644920Z","shell.execute_reply.started":"2021-12-05T13:01:35.074881Z","shell.execute_reply":"2021-12-05T13:01:48.643940Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# padding tokenized texts to the same MAX_LEN\nMAX_LEN = max(df.tok_length)\nprint(MAX_LEN)\n\ndef padding(tokenized, MAX_LEN):\n    original = tokenized.copy()\n    pads = [0] * (MAX_LEN - len(tokenized))\n    return original + pads\n\ndf['padded'] = df.tokenized.apply(lambda x: padding(x, MAX_LEN))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:01:48.646287Z","iopub.execute_input":"2021-12-05T13:01:48.646577Z","iopub.status.idle":"2021-12-05T13:01:49.175370Z","shell.execute_reply.started":"2021-12-05T13:01:48.646539Z","shell.execute_reply":"2021-12-05T13:01:49.174324Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(df, random_state=2021 , stratify=df.type)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:01:49.176953Z","iopub.execute_input":"2021-12-05T13:01:49.177327Z","iopub.status.idle":"2021-12-05T13:01:49.209655Z","shell.execute_reply.started":"2021-12-05T13:01:49.177268Z","shell.execute_reply":"2021-12-05T13:01:49.208776Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Build Dataset","metadata":{}},{"cell_type":"code","source":"X_train = []\nX_test = []\n\nfor i, tok in enumerate(train.padded.values):\n    X_train.append(tok)\n    \nfor i, tok in enumerate(test.padded.values):\n    X_test.append(tok)\n\ny_train = train.is_I.to_numpy()\ny_test = test.is_I.to_numpy()\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\n\nassert X_train.shape[0] == len(y_train)\nassert X_test.shape[0] == len(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:11:59.778116Z","iopub.execute_input":"2021-12-05T13:11:59.778441Z","iopub.status.idle":"2021-12-05T13:12:01.482728Z","shell.execute_reply.started":"2021-12-05T13:11:59.778407Z","shell.execute_reply":"2021-12-05T13:12:01.481522Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n\n# dataloaders\nbatch_size = 30\n\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:12:02.971162Z","iopub.execute_input":"2021-12-05T13:12:02.971738Z","iopub.status.idle":"2021-12-05T13:12:02.980234Z","shell.execute_reply.started":"2021-12-05T13:12:02.971683Z","shell.execute_reply":"2021-12-05T13:12:02.979040Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, nlayers, outsize, vsize, hsize, emsize, drop_prob=0.5):\n        super(RNN, self).__init__()\n        self.outsize = outsize\n        self.hsize = hsize\n        self.nlayers = nlayers\n        self.vsize = vsize\n        self.embedding = nn.Embedding(vsize, emsize)\n        self.lstm = nn.LSTM(input_size=emsize, hidden_size=hsize, num_layers=nlayers, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(self.hsize, outsize)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x, hidden):\n        # print(\"x.shape: \", x.shape)\n        # print(\"x.grad: \", x.grad)\n        batch_size = x.size(0)\n        embeds = self.embedding(x)\n        self.embedding.weight.retain_grad()\n#         lstm_out, hidden = self.lstm(embeds)\n#         lstm_out = embeds.mean(1)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hsize)\n\n        # dropout and fully connected layer\n        out = self.fc(self.dropout(lstm_out))\n\n        # sigmoid function\n        sig_out = self.sig(out)\n\n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        return sig_out, hidden\n\n#     def init_hidden(self, batch_size):\n#         ''' Initializes hidden state '''\n#         # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n#         # initialized to zero, for hidden state and cell state of LSTM\n#         h0 = torch.zeros((self.nlayers, batch_size, self.hsize)).to(device)\n#         c0 = torch.zeros((self.nlayers, batch_size, self.hsize)).to(device)\n#         hidden = (h0, c0)\n#         return hidden\n    \n    def init_hidden(self, batchsize):\n        weight = next(self.parameters())\n        return (\n            weight.new_zeros(self.nlayers, batchsize, self.hsize),\n            weight.new_zeros(self.nlayers, batchsize, self.hsize)\n        )","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:12:04.008093Z","iopub.execute_input":"2021-12-05T13:12:04.008729Z","iopub.status.idle":"2021-12-05T13:12:04.021872Z","shell.execute_reply.started":"2021-12-05T13:12:04.008676Z","shell.execute_reply":"2021-12-05T13:12:04.020537Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"nlayers = 2\nvsize = len(voc)\nemsize = 100\noutsize = 1\nhsize = 100\n\nmodel = RNN(nlayers, outsize, vsize, hsize, emsize)\n\n#moving to gpu\nmodel.to(device)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:12:04.675271Z","iopub.execute_input":"2021-12-05T13:12:04.675809Z","iopub.status.idle":"2021-12-05T13:12:04.708325Z","shell.execute_reply.started":"2021-12-05T13:12:04.675777Z","shell.execute_reply":"2021-12-05T13:12:04.707322Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# loss and optimization functions\nlr = 0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n    pred = torch.round(pred.squeeze())\n    return torch.sum(pred == label.squeeze()).item()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:12:07.325089Z","iopub.execute_input":"2021-12-05T13:12:07.325676Z","iopub.status.idle":"2021-12-05T13:12:07.331793Z","shell.execute_reply.started":"2021-12-05T13:12:07.325643Z","shell.execute_reply":"2021-12-05T13:12:07.330468Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"clip = 5\nepochs = 30\nvalid_loss_min = np.Inf\n# train for some number of epochs\nepoch_tr_loss, epoch_vl_loss = [], []\nepoch_tr_acc, epoch_vl_acc = [], []\n\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    # initialize hidden state\n    h = model.init_hidden(batch_size)\n    print(\"h1: \", h[0].shape, h[1].shape)\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        model.zero_grad()\n        output, h = model(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        train_losses.append(loss.item())\n        # calculating accuracy\n        accuracy = acc(output, labels)\n        train_acc += accuracy\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n    val_h = model.init_hidden(batch_size)\n    val_losses = []\n    val_acc = 0.0\n    model.eval()\n    for inputs, labels in valid_loader:\n        val_h = tuple([each.data for each in val_h])\n        inputs, labels = inputs.to(device), labels.to(device)\n        output, val_h = model(inputs, val_h)\n        val_loss = criterion(output.squeeze(), labels.float())\n        val_losses.append(val_loss.item())\n        accuracy = acc(output, labels)\n        val_acc += accuracy\n\n    epoch_train_loss = np.mean(train_losses)\n    epoch_val_loss = np.mean(val_losses)\n    epoch_train_acc = train_acc/len(train_loader.dataset)\n    epoch_val_acc = val_acc/len(valid_loader.dataset)\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}')\n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(\n        f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '../working/is_I.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min, epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:12:11.656723Z","iopub.execute_input":"2021-12-05T13:12:11.659607Z","iopub.status.idle":"2021-12-05T13:12:38.133412Z","shell.execute_reply.started":"2021-12-05T13:12:11.659561Z","shell.execute_reply":"2021-12-05T13:12:38.132064Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Combine","metadata":{}},{"cell_type":"code","source":"models = {}\nmodels['F'] = RNN(nlayers, outsize, vsize, hsize, emsize)\nmodels['J'] = RNN(nlayers, outsize, vsize, hsize, emsize)\nmodels['N'] = RNN(nlayers, outsize, vsize, hsize, emsize)\nmodels['I'] = RNN(nlayers, outsize, vsize, hsize, emsize)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:13:01.233350Z","iopub.execute_input":"2021-12-05T13:13:01.233699Z","iopub.status.idle":"2021-12-05T13:13:01.315112Z","shell.execute_reply.started":"2021-12-05T13:13:01.233669Z","shell.execute_reply":"2021-12-05T13:13:01.314121Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"models['F'].load_state_dict(torch.load('../working/is_F.pt'))\nmodels['J'].load_state_dict(torch.load('../working/is_J.pt'))\nmodels['N'].load_state_dict(torch.load('../working/is_N.pt'))\nmodels['I'].load_state_dict(torch.load('../working/is_I.pt'))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:13:30.945079Z","iopub.execute_input":"2021-12-05T13:13:30.945390Z","iopub.status.idle":"2021-12-05T13:13:30.984627Z","shell.execute_reply.started":"2021-12-05T13:13:30.945340Z","shell.execute_reply":"2021-12-05T13:13:30.983357Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def predict(model, text):\n    model.eval()\n    if isinstance(text, str):\n        padded_text = np.array(padding(voc(tokenizer(text)), MAX_LEN)).reshape(1,-1)\n        feature_tensor = torch.from_numpy(padded_text)\n    else:\n        feature_tensor = torch.from_numpy(np.array(text).reshape(1,-1))\n    batch_size = 1\n    h = model.init_hidden(batch_size)\n    h = tuple([each.data for each in h])\n    output, h = model(feature_tensor, h)\n    return(output.item())","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:24:29.277087Z","iopub.execute_input":"2021-12-05T13:24:29.279670Z","iopub.status.idle":"2021-12-05T13:24:29.297379Z","shell.execute_reply.started":"2021-12-05T13:24:29.279601Z","shell.execute_reply":"2021-12-05T13:24:29.292841Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"s = \"this is a stupid test let see if it works!\"\npredict(models['N'], df.padded[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:24:34.496646Z","iopub.execute_input":"2021-12-05T13:24:34.496967Z","iopub.status.idle":"2021-12-05T13:24:34.509528Z","shell.execute_reply.started":"2021-12-05T13:24:34.496938Z","shell.execute_reply":"2021-12-05T13:24:34.507925Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, multilabel_confusion_matrix\nfrom sklearn.metrics import roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:37:23.058055Z","iopub.execute_input":"2021-12-05T13:37:23.058495Z","iopub.status.idle":"2021-12-05T13:37:23.066215Z","shell.execute_reply.started":"2021-12-05T13:37:23.058462Z","shell.execute_reply":"2021-12-05T13:37:23.065092Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def get_all4(tokenized_input):\n    out = np.zeros(4)\n    idx = {\"I\":0, \"N\":1, \"F\":2, \"J\":3}\n    for key, model in models.items():\n        out[idx[key]] = predict(model, tokenized_input)\n    return out > 0.5","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:27:37.970317Z","iopub.execute_input":"2021-12-05T13:27:37.970623Z","iopub.status.idle":"2021-12-05T13:27:37.976433Z","shell.execute_reply.started":"2021-12-05T13:27:37.970593Z","shell.execute_reply":"2021-12-05T13:27:37.975556Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"predicted = []\nfor each in test.padded:\n    predicted.append(get_all4(each))\npredicted = np.array(predicted)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:30:04.849872Z","iopub.execute_input":"2021-12-05T13:30:04.850133Z","iopub.status.idle":"2021-12-05T13:30:09.438962Z","shell.execute_reply.started":"2021-12-05T13:30:04.850105Z","shell.execute_reply":"2021-12-05T13:30:09.437833Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"auroc = {}\nauroc[\"I\"] = roc_auc_score(test.is_I.to_numpy(), predicted[:,0])\nauroc[\"N\"] = roc_auc_score(test.is_I.to_numpy(), predicted[:,1])\nauroc[\"F\"] = roc_auc_score(test.is_I.to_numpy(), predicted[:,2])\nauroc[\"J\"] = roc_auc_score(test.is_I.to_numpy(), predicted[:,3])\nauroc","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:39:10.566788Z","iopub.execute_input":"2021-12-05T13:39:10.567066Z","iopub.status.idle":"2021-12-05T13:39:10.589861Z","shell.execute_reply.started":"2021-12-05T13:39:10.567035Z","shell.execute_reply":"2021-12-05T13:39:10.588820Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"targets = [test.is_I.to_numpy(), test.is_N.to_numpy(), test.is_F.to_numpy(), test.is_J.to_numpy()]\ntargets = np.array(targets).T\nresult = np.prod(targets == predicted, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:45:35.475732Z","iopub.execute_input":"2021-12-05T13:45:35.476022Z","iopub.status.idle":"2021-12-05T13:45:35.482103Z","shell.execute_reply.started":"2021-12-05T13:45:35.475993Z","shell.execute_reply":"2021-12-05T13:45:35.480988Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"print(classification_report(\n  targets, \n  predicted,\n  zero_division=0\n))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T13:47:19.067272Z","iopub.execute_input":"2021-12-05T13:47:19.067662Z","iopub.status.idle":"2021-12-05T13:47:19.095527Z","shell.execute_reply.started":"2021-12-05T13:47:19.067634Z","shell.execute_reply":"2021-12-05T13:47:19.094449Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}